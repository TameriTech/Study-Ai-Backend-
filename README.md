# ğŸ“š Study AI â€“ Plateforme d'apprentissage intelligente

![Python](https://img.shields.io/badge/python-3.12-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green)
![Status](https://img.shields.io/badge/status-en%20cours-yellow)

---

## ğŸ“‘ Table des matiÃ¨res

- [PrÃ©sentation](#-study-ai--plateforme-dapprentissage-intelligente)
- [Objectifs](#-objectifs)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s-principales)
- [Technologies](#-technologies-utilisÃ©es)
- [Architecture](#-architecture-du-projet)
- [Structure](#-structure-du-projet-studyai)
- [Contraintes](#-contraintes--dÃ©fis)
- [CritÃ¨res de validation](#-critÃ¨res-de-validation)
- [Tests](#-tests)
- [Installation & Lancement](#-lancement-rapide-dev)
- [Fichier .env](#-exemple-de-fichier-env)
- [Contribution](#-contribuer)
- [Contact](#-auteurs--contribution)

---

## ğŸ§  PrÃ©sentation

**Study AI** est une application innovante dÃ©veloppÃ©e par **Tameri Tech** pour transformer l'apprentissage des Ã©tudiants. Elle permet d'automatiser l'analyse de contenus pÃ©dagogiques (PDF, vidÃ©os) grÃ¢ce Ã  l'intelligence artificielle, et gÃ©nÃ¨re des fiches de rÃ©vision et des quiz personnalisÃ©s.

---

## ğŸš€ Objectifs

- Offrir une solution interactive pour rÃ©viser efficacement
- GÃ©nÃ©rer automatiquement des contenus Ã  partir de documents/vidÃ©os
- AmÃ©liorer la pertinence via l'IA et les retours utilisateurs

---

## ğŸ§© FonctionnalitÃ©s principales

- ğŸ“„ **PDF** : Importation, extraction, analyse de contenu
- ğŸ¥ **VidÃ©os** : Traitement image, reconnaissance vocale
- ğŸ–‹ï¸ **GÃ©nÃ©ration automatique** : Fiches de rÃ©vision, quiz personnalisÃ©s
- ğŸ¤– **IA & Feedback** : AmÃ©lioration continue, retours utilisateurs
- ğŸ“Š **Dashboard interactif** : AccÃ¨s aux fichiers, fiches et statistiques

---

## ğŸ› ï¸ Technologies utilisÃ©es

| Composant       | Technologies                                                  |
|----------------|---------------------------------------------------------------|
| Backend         | Python, FastAPI                                               |
| Frontend        | Kotlin (Java) *(optionnel)*                                   |
| Base de donnÃ©es | PostgreSQL                                                    |
| PDF             | PDFMiner, Apache Tika                                         |
| VidÃ©o & Audio   | OpenCV, Google/Azure Speech-to-Text                           |
| IA              | OpenAI API (GPT-4), TensorFlow / PyTorch                      |

---

## ğŸ“Š Architecture du projet

L'application suit une **architecture modulaire en couches** :

- Routes FastAPI (pdf, vidÃ©os, quiz, feedback, user)
- Services de traitement (OCR, NLP, IA, audio)
- Moteur de gÃ©nÃ©ration de quiz & fiches
- SystÃ¨me de feedback
- Base de donnÃ©es et modÃ¨les ORM

---

## ğŸ“ Structure du projet StudyAI

```bash

Study-Ai-Backend/
â”‚
â”œâ”€â”€ api/                          # Contains all route/controller logic
â”‚   â”œâ”€â”€ courses.py
â”‚   â”œâ”€â”€ documents.py
â”‚   â”œâ”€â”€ feedback.py
â”‚   â”œâ”€â”€ quiz.py
â”‚   â”œâ”€â”€ segments.py
â”‚   â”œâ”€â”€ users.py
â”‚   â””â”€â”€ vocabulary.py
â”‚
â”œâ”€â”€ database/                     # Database layer: connection, models, and schemas
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ services/                     # Business logic and service layer
â”‚   â”œâ”€â”€ course_service.py
â”‚   â”œâ”€â”€ document_service.py
â”‚   â”œâ”€â”€ feedback_service.py
â”‚   â”œâ”€â”€ quiz_service.py
â”‚   â”œâ”€â”€ segment_service.py
â”‚   â”œâ”€â”€ users_services.py
â”‚   â””â”€â”€ vocabulary_services.py
â”‚
â”œâ”€â”€ utils/                        # Utility functions for reusability
â”‚   â”œâ”€â”€ general_utils.py
â”‚   â”œâ”€â”€ image_util.py
â”‚   â”œâ”€â”€ ollama_utils.py
â”‚   â”œâ”€â”€ pdf_util.py
â”‚   â””â”€â”€ video_util.py
â”‚
â”œâ”€â”€ temp_files/                   # Temporary storage for uploaded or processed files
â”‚   â”œâ”€â”€ images/                   # Temporary image files
â”‚   â”œâ”€â”€ pdf/                      # Temporary PDF documents
â”‚   â””â”€â”€ videos/                   # Temporary video files
â”‚
â”œâ”€â”€ .env                          # Environment variables
â”œâ”€â”€ .gitignore                    # Git ignored files/folders
â”œâ”€â”€ api_documentation.md         # API documentation
â”œâ”€â”€ main.py                       # Application entry point
â”œâ”€â”€ README.md                     # Project description and instructions
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ TODO.md                       # Tasks and future enhancements
```

---

## âš ï¸ Contraintes & dÃ©fis

- Gestion de PDF variÃ©s (avec code, images, etc.)
- QualitÃ© des vidÃ©os/audio parfois faible
- Temps de traitement de gros fichiers
- Interface Ã  la fois intuitive et riche

---

## âœ… CritÃ¨res de validation

| FonctionnalitÃ©       | CritÃ¨re attendu                                     |
|----------------------|------------------------------------------------------|
| PDF                  | PrÃ©cision > 95%                                     |
| VidÃ©o               | Concepts extraits dans > 90% des cas                 |
| Quiz                 | Pertinence du contenu gÃ©nÃ©rÃ©                       |
| Performance          | Temps de rÃ©ponse acceptable (mÃªme fichiers lourds)   |
| Feedback utilisateur | SystÃ¨me intuitif, utilisÃ© activement                |

---

## ğŸ§ª Tests

- ğŸ”¬ **Unitaires** : PDFImporter, VideoAnalyzer, QuizGenerator
- ğŸ”— **IntÃ©gration** : ChaÃ®ne complÃ¨te (import â†’ quiz)
- ğŸ” **Fonctionnels** : Cas dâ€™usage rÃ©els
- ğŸ“Š **Performance** : ScalabilitÃ©, charge
- ğŸ‘¥ **Utilisateurs** : Feedback humain pour ajustement

---

## ğŸ Lancement rapide (dev)

```bash
# Cloner le projet
git clone https://github.com/TameriTech/Study-Ai-Backend-.git
cd study-ai

# CrÃ©er l'environnement virtuel
python -m venv env
source env/Scripts/activate

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer l'application
uvicorn app.main:app --reload
```

---

## ğŸ” Exemple de fichier `.env`

```env
OPENAI_API_KEY=sk-xxx
GOOGLE_SPEECH_API_KEY=xxx
DATABASE_URL=postgresql://user:password@localhost/studyai
```

---

## ğŸ¤ Contribuer

Les contributions sont les bienvenues !

- Forkez le repo
- CrÃ©ez une branche (`git checkout -b feature/ma-feature`)
- Commitez vos changements
- Push (`git push origin feature/ma-feature`)
- Ouvrez une pull request ğŸš€

---

## ğŸ“¢ Auteurs & Contribution

**DÃ©veloppÃ© par :** Tameri Tech  
**Contact :** [tameri.tech25@gmail.com](mailto:tameri.tech25@gmail.com)

> Diagrammes UML disponibles dans le rapport dâ€™analyse officiel du projet.

---

**âœ¨ Rejoignez la rÃ©volution de l'apprentissage intelligent avec Study AI.**

=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================

USER
â”€â”€â”€â”€â”€â”€
â€¢ id
â€¢ fullName
â€¢ email
â€¢ password
â€¢ best_subjects
â€¢ learning_objectives
â€¢ class_level
â€¢ academic_level
â€¢ statistic
â€¢ created_at
â”‚
â””â”€â”€â”€ 1:* â”€â”€â”€â–º DOCUMENT
              â”€â”€â”€â”€â”€â”€
              â€¢ id
              â€¢ title
              â€¢ type_document
              â€¢ original_filename
              â€¢ storage_path
              â€¢ original_text
              â€¢ uploaded_at
              â€¢ user_id
              â”‚
              â”œâ”€â”€â”€ 1:* â”€â”€â”€â–º COURSE
              â”‚            â”€â”€â”€â”€â”€â”€
              â”‚            â€¢ id
              â”‚            â€¢ course_name
              â”‚            â€¢ original_text
              â”‚            â€¢ simplified_text
              â”‚            â€¢ summary_text
              â”‚            â€¢ level
              â”‚            â€¢ estimated_completion_time
              â”‚            â€¢ summary_module
              â”‚            â€¢ simplified_modules
              â”‚            â€¢ simplified_module_pages
              â”‚            â€¢ summary_modules_pages
              â”‚            â€¢ simplified_current_page
              â”‚            â€¢ summary_current_page
              â”‚            â€¢ simplified_module_statistic
              â”‚            â€¢ summary_modules_statistic
              â”‚            â€¢ document_id
              â”‚            â€¢ created_at
              â”‚            â”‚
              â”‚            â”œâ”€â”€â”€ 1:* â”€â”€â”€â–º QUIZ
              â”‚            â”‚            â”€â”€â”€â”€â”€â”€
              â”‚            â”‚            â€¢ id
              â”‚            â”‚            â€¢ instruction
              â”‚            â”‚            â€¢ question
              â”‚            â”‚            â€¢ correct_answer
              â”‚            â”‚            â€¢ choices
              â”‚            â”‚            â€¢ quiz_type
              â”‚            â”‚            â€¢ level_of_difficulty
              â”‚            â”‚            â€¢ number_of_questions
              â”‚            â”‚            â€¢ created_at
              â”‚            â”‚            â€¢ course_id
              â”‚            â”‚            â”‚
              â”‚            â”‚            â””â”€â”€â”€ 1:1 â”€â”€â”€â–º FEEDBACK
              â”‚            â”‚                         â”€â”€â”€â”€â”€â”€
              â”‚            â”‚                         â€¢ id
              â”‚            â”‚                         â€¢ rating
              â”‚            â”‚                         â€¢ comment
              â”‚            â”‚                         â€¢ created_at
              â”‚            â”‚                         â€¢ quiz_id
              â”‚            â”‚
              â”‚            â””â”€â”€â”€ 1:1 â”€â”€â”€â–º VOCABULARY
              â”‚                          â”€â”€â”€â”€â”€â”€
              â”‚                          â€¢ id
              â”‚                          â€¢ words [{term:"", definition:""}]
              â”‚                          â€¢ course_id
              â”‚
              â””â”€â”€â”€ 1:* â”€â”€â”€â–º SEGMENT
                          â”€â”€â”€â”€â”€â”€
                          â€¢ id
                          â€¢ raw_text
                          â€¢ embedding_vector
                          â€¢ created_at
                          â€¢ document_id


=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================

Here's a clear and professional documentation for the `users_services` module based on your provided code. This can be added to your `api_documentation.md` or kept separately under a **Services Documentation** section.

---

## ğŸ§© `users_services.py` â€“ User Service Logic

This module handles **CRUD operations** and **authentication logic** for users.

### ğŸ”’ Authentication

```python
def authenticate_user(db: Session, email: str, password: str)
```

- **Purpose**: Validates user credentials.
- **Parameters**:
  - `db`: SQLAlchemy DB session.
  - `email` *(str)*: User's email address.
  - `password` *(str)*: Plain-text password to verify.
- **Returns**: User object if credentials are valid, otherwise `None`.

---

### ğŸ§‘ Create User

```python
def create_user(db: Session, data: UserCreate)
```

- **Purpose**: Creates a new user after checking for email uniqueness.
- **Parameters**:
  - `db`: SQLAlchemy DB session.
  - `data`: Pydantic schema `UserCreate` containing user details.
- **Logic**:
  - Checks if the email already exists.
  - Hashes the password before storing.
- **Raises**: `HTTPException` with status `400` if email already exists.
- **Returns**: The newly created user object.

---

### ğŸ“¥ Get All Users

```python
def get_users(db: Session)
```

- **Purpose**: Fetches all users from the database.
- **Returns**: List of user objects.

---

### ğŸ‘¤ Get a Single User

```python
def get_user(db: Session, user_id: int)
```

- **Purpose**: Fetches a user by their ID.
- **Returns**: A single user object or `None` if not found.

---

### âœï¸ Update User

```python
def update_user(db: Session, user: UserCreate, user_id: int)
```

- **Purpose**: Updates user fields with new data.
- **Parameters**:
  - `db`: SQLAlchemy DB session.
  - `user`: New user data (as `UserCreate` schema).
  - `user_id`: ID of the user to update.
- **Logic**: Dynamically updates fields using `model_dump()`.
- **Returns**: The updated user object or `None` if not found.

---

### âŒ Delete User

```python
def delete_user(db: Session, user_id: int)
```

- **Purpose**: Deletes a user by their ID.
- **Returns**: The deleted user object or `None` if not found.

---

=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================
Here's the comprehensive documentation Documents module following your established style:

---

## ğŸ“„ `documents services` â€“ Document Processing Service

Handles file uploads (PDFs, images, videos), text extraction, and initial processing pipeline including:
- File storage management
- Text extraction (OCR for images/videos)
- AI-powered summarization/simplification
- Automatic course creation
- Text segmentation with embeddings

---

### ğŸ“‘ **PDF Processing**
```python
async def extract_and_save_pdf(db: Session, file: UploadFile, user_id: int) -> dict
```

#### Features
- Validates PDF files
- Extracts raw text using PyMuPDF
- Generates AI summaries and simplifications
- Creates database records and initiates processing pipeline

#### Parameters
| Parameter | Type          | Description                          |
|-----------|---------------|--------------------------------------|
| `db`      | `Session`     | SQLAlchemy database session          |
| `file`    | `UploadFile`  | PDF file to process                  |
| `user_id` | `int`         | Owner's user ID                      |

#### Returns
```json
{
  "document_id": 123,
  "user_id": 456,
  "filename": "notes.pdf",
  "storage_path": "temp_files/pdf/20240101_123456_notes.pdf",
  "extracted_text": "Lorem ipsum...",
  "message": "PDF processed successfully..."
}
```

#### Error Cases
- `400 Bad Request`: Non-PDF file uploaded
- `500 Internal Server Error`: File processing failures

---

### ğŸ“¸ **Image Processing**
```python
async def extract_and_save_image(db: Session, file: UploadFile, user_id: int) -> dict
```

#### Features
- Accepts common image formats (JPEG, PNG, etc.)
- Performs OCR using Tesseract
- Parallel processing pipeline to PDF documents

#### Special Considerations
```python
# Requires Tesseract OCR installed:
# sudo apt install tesseract-ocr  # Linux
# brew install tesseract           # macOS
```

#### Error Cases
- `400 Bad Request`: Non-image file uploaded
- `500 Internal Server Error`: OCR processing failures

---

### ğŸ¥ **Video Processing**
```python
async def extract_and_save_video(
    db: Session, 
    file: UploadFile, 
    user_id: int, 
    frames_per_second: int = 1
) -> dict
```

#### Features
- Supports MP4, MOV, AVI, MKV
- Frame extraction via FFmpeg
- Per-frame OCR processing
- Configurable FPS for performance/coverage tradeoff

#### Dependencies
```bash
# Requires FFmpeg:
# sudo apt install ffmpeg  # Linux
# brew install ffmpeg      # macOS
```

#### Error Cases
- `400 Bad Request`: Unsupported video format
- `500 Internal Server Error`: FFmpeg/OCR processing failures

---

### ğŸ”„ **Common Processing Pipeline**
All methods follow this workflow:
1. File validation â†’ 2. Storage â†’ 3. Text extraction â†’  
4. AI processing â†’ 5. Database records â†’ 6. Course/Segment creation

#### Shared Return Structure
All successful operations return:
- Document metadata
- Extracted text sample
- Processing confirmation
- Generated course ID

#### Error Handling
Uniform error responses with:
- Machine-readable status codes
- Human-friendly error messages
- Contextual details for debugging

---

### ğŸ›  **Utility Functions**
| Function                | Description                                  |
|-------------------------|----------------------------------------------|
| `_save_to_temp()`       | Handles secure file storage with timestamped names |
| `_validate_file_type()` | Checks file extensions and MIME types        |

---

=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================
Here's the comprehensive documentation for your `course_service.py` module:

---

## ğŸ“š `course_service.py` â€“ Course Management Service

Handles course creation, module processing, progress tracking, and search functionality for your learning platform.

---

### â• **Create Course**
```python
def create_course(
    db, 
    document_id: int,
    course_name: str,
    original_text: Optional[str] = None,
    simplified_text: Optional[str] = None,
    summary_text: Optional[str] = None,
    level: str = "beginner"
) -> Course
```

#### Features
- Automatically structures content into modules using AI
- Generates estimated completion time
- Creates both simplified and summary versions
- Calculates initial pagination statistics

#### AI Processing Pipeline
1. **Module Generation**:
   ```python
   simplified_modules = generate_from_ollama(prompt)  # For both simplified/summary
   ```
2. **Time Estimation**:
   ```python
   estimated_time = generate_from_ollama("...text...")  # <12 character response
   ```

#### Returns
`Course` object with:
- Structured modules (JSON)
- Pagination counts
- Original/processed content
- Completion metrics

---

### ğŸ” **Module Retrieval**
| Function | Description |
|----------|-------------|
| `get_simplified_modules()` | Gets simplified modules by document ID |
| `get_summary_modules()` | Gets summary modules by document ID |
| `get_simplified_modules_by_course_id()` | Gets modules by course ID |
| `get_summary_modules_by_course_id()` | Gets summary by course ID |

**All functions return**:  
`List[Dict]` or `None` if not found

---

### ğŸ“ˆ **Progress Tracking**
```python
def update_simplified_progress(db, course_id, current_page) -> Course
def update_summary_progress(db, course_id, current_page) -> Course
```

#### Features
- Auto-calculates completion percentage
- Prevents page overflow
- Updates both current page and statistics

**Example**:
```python
# Updates page 3 of 10 â†’ 30% completion
update_simplified_progress(db, 123, 3)  
```

---

### ğŸ—‚ **Course Access**
| Function | Description |
|----------|-------------|
| `get_course_from_db()` | Gets full course by ID |
| `get_user_courses()` | Lists all courses for a user |

**Returns**:  
- Single `Course` object or `None`
- List of courses (SQLAlchemy objects)

---

### ğŸ” **Advanced Search**
```python
def search_courses(
    db,
    search_query: str,
    min_query_length=2,
    limit=10,
    skip=0,
    search_fields=None,
    fuzzy_match=False
) -> dict
```

#### Features
- Field-specific searching (default: course_name)
- Fuzzy matching (PG trigram similarity)
- Pagination support

**Search Options**:
```python
# Exact match on multiple fields
search_courses(db, "math", search_fields=["course_name", "original_text"])

# Fuzzy matching
search_courses(db, "algebr", fuzzy_match=True)
```

**Return Structure**:
```json
{
  "results": [Course1, Course2],
  "pagination": {
    "total": 15,
    "returned": 2,
    "skip": 0,
    "limit": 10
  }
}
```

#### Error Cases
- `400 Bad Request`: Query too short
- `400 Bad Request`: Invalid search fields

---

### ğŸ›  **Internal Utilities**
| Function | Description |
|----------|-------------|
| `parse_modules()` | Converts AI output to structured modules |
| `_calculate_progress()` | Handles percentage calculations |

---

=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================
Here's the documentation for your `segment_service.py` module:

---

## ğŸ” `segment_service.py` â€“ Text Segmentation & Embedding Service

Handles text chunking and vector embedding generation for semantic search and content analysis.

---

### âš™ï¸ **Core Function**
```python
def process_segments(
    db: Session, 
    document_id: int, 
    text: str, 
    chunk_size: int = 1000
) -> int
```

#### Features
- Splits text into manageable chunks
- Generates 384-dimension embeddings using `all-MiniLM-L6-v2` model
- Stores embeddings as JSON-serialized arrays
- Skips empty/whitespace chunks
- Returns count of created segments

#### Parameters
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `db` | `Session` | - | SQLAlchemy session |
| `document_id` | `int` | - | Parent document ID |
| `text` | `str` | - | Raw content to process |
| `chunk_size` | `int` | 1000 | Character length per segment |

#### Technical Details
```python
# Embedding Generation
embedding = model.encode(text_chunk)  # Returns numpy array
vector = json.dumps(embedding.tolist())  # Serialized storage

# Chunking Logic
chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
```

#### Returns
Integer count of successfully created segments

---

### ğŸ§® **Embedding Specifications**
| Model | Dimensions | Size | Best For |
|-------|------------|------|----------|
| `all-MiniLM-L6-v2` | 384 | ~80MB | Fast semantic search |

---

### ğŸ’¾ **Database Storage**
```python
class Segment(BaseModel):
    embedding_vector: str  # JSON string of float array
    raw_text: str         # Original chunk content
    document_id: int      # Foreign key
```

#### Retrieval Example
```python
# Get embedding back to numpy array
segment = db.query(Segment).first()
embedding = np.array(json.loads(segment.embedding_vector))
```

---

### âš ï¸ **Requirements**
1. SentenceTransformers installed:
   ```bash
   pip install sentence-transformers
   ```
2. NumPy for array handling

---

### ğŸ“Š **Performance Considerations**
- **Chunk Size**: 500-1500 chars recommended
- **Memory**: ~1MB per 1000 segments
- **Processing**: ~100ms per chunk on CPU

Would you like me to:
1. Add examples of querying with embeddings?
2. Document the model selection tradeoffs?
3. Include error handling recommendations?

### ğŸ”„ **Data Flow**
1. Document uploaded â†’ 2. Course created â†’ 3. Modules generated â†’  
4. User interacts â†’ 5. Progress updated â†’ 6. Searchable content

=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================
Here's the comprehensive documentation for your Vocabulary API endpoints:

---

## ğŸ“– Vocabulary API Endpoints

### **Base URL**
`/api/vocabularies`

---

### â• Create Vocabulary Entry
`POST /api/create-vocabularies/{course_id}/`

#### **Description**
Creates a new vocabulary entry for a specific course.

#### **Parameters**
| Parameter | Type | Location | Required | Description |
|-----------|------|----------|----------|-------------|
| `course_id` | integer | path | Yes | ID of the associated course |

#### **Responses**
- `200 OK`: Returns the created vocabulary entry
  ```json
  {
    "id": 1,
    "words": [
      {"term": "algorithm", "definition": "A set of rules..."}
    ],
    "course_id": 5
  }
  ```
- `500 Internal Server Error`: Creation failed
  ```json
  {
    "detail": "Error message"
  }
  ```

---

### ğŸ“š Get Vocabulary Words
`GET /api/vocabularies/{course_id}/words`

#### **Description**
Retrieves all vocabulary words for a specific course.

#### **Parameters**
| Parameter | Type | Location | Required | Description |
|-----------|------|----------|----------|-------------|
| `course_id` | integer | path | Yes | ID of the course |

#### **Responses**
- `200 OK`: Returns vocabulary words
  ```json
  {
    "words": [
      {"term": "variable", "definition": "A storage location..."},
      {"term": "function", "definition": "A reusable code block..."}
    ]
  }
  ```
- `500 Internal Server Error`: Retrieval failed
  ```json
  {
    "detail": "Error retrieving words: [error details]"
  }
  ```

---

### ğŸ” Search Vocabulary
`GET /api/vocabularies/{course_id}/search`

#### **Description**
Searches for vocabulary terms within a course.

#### **Parameters**
| Parameter | Type | Location | Required | Description |
|-----------|------|----------|----------|-------------|
| `course_id` | integer | path | Yes | ID of the course |
| `keyword` | string | query | Yes | Search term (min 1 character) |

#### **Responses**
- `200 OK`: Returns matching vocabulary words
  ```json
  {
    "words": [
      {"term": "database", "definition": "An organized collection..."}
    ]
  }
  ```
- `500 Internal Server Error`: Search failed
  ```json
  {
    "detail": "Search error: [error details]"
  }
  ```

---

### ğŸ·ï¸ Data Schemas
#### **Vocabulary**
```json
{
  "id": "integer",
  "words": [
    {
      "term": "string",
      "definition": "string"
    }
  ],
  "course_id": "integer"
}
```

#### **VocabularyWords**
```json
{
  "words": [
    {
      "term": "string",
      "definition": "string"
    }
  ]
}
```

---

### Error Handling
All endpoints return standardized error responses:
- `500` status code for server errors
- JSON-formatted error details
- Original error message in development mode

---

### Example Usage
```bash
# Create vocabulary
curl -X POST http://localhost:8000/api/create-vocabularies/5/

# Get words
curl http://localhost:8000/api/vocabularies/5/words

# Search words
curl http://localhost:8000/api/vocabularies/5/search?keyword=variable
```


=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================

Here's the comprehensive API documentation for your StudyAI backend routes:

---

# ğŸ“š StudyAI API Documentation

## ğŸ” **Authentication**
`POST /api/login`  
**Description**: Authenticate user and get JWT token  
**Request Body**:
```json
{
  "email": "user@example.com",
  "password": "securepassword"
}
```
**Response**:
```json
{
  "access_token": "eyJhbGci...",
  "token_type": "bearer"
}
```

---

## ğŸ‘¥ **User Management**
| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/api/register` | POST | Register new user | `UserCreate` schema |
| `/api/get-users` | GET | List all users | - |
| `/api/get-user/{id}` | GET | Get user by ID | `id` (path) |
| `/api/user/update/{id}` | PUT | Update user | `id` (path), `UserCreate` schema |
| `/api/delete/user/{id}` | DELETE | Delete user | `id` (path) |

---

## ğŸ“„ **Document Processing**
### PDF Processing
`POST /api/extract-pdf-text`  
**Description**: Upload and process PDF  
**Parameters**:
- `file`: PDF file (UploadFile)
- `user_id`: Owner ID (query)

**Response**:
```json
{
  "document_id": 123,
  "extracted_text": "First 100 chars...",
  "storage_path": "temp_files/pdf/...",
  "simplified_text": "Simplified version...",
  "summary_text": "Condensed summary..."
}
```

### Image Processing
`POST /api/extract-text-from-image`  
**Tech Stack**: Uses Pytesseract for OCR  
**Error Cases**:
- `400`: Non-image file
- `500`: OCR processing failure

### Video Processing
`POST /api/extract-text-from-video`  
**Tech Stack**: FFmpeg (frame extraction) + Pytesseract (OCR)  
**Parameters**:
- `frames_per_second`: 1-10 (default: 1)

---

## ğŸ“š **Course Management**
### Course Retrieval
| Endpoint | Description | Response |
|----------|-------------|----------|
| `GET /api/get-course/{course_id}` | Get full course | `Course` object |
| `GET /api/user/{user_id}/courses` | List user's courses | `List[Course]` |

### Module Access
```mermaid
graph LR
    A[Document] --> B(Simplified Modules)
    A --> C(Summary Modules)
    B --> D[GET /courses-doc/{id}/simplified-modules]
    C --> E[GET /courses-doc/{id}/summary-modules]
```

### Progress Tracking
`PUT /api/course/{course_id}/update-simplified-progress`  
**Body**:
```json
{"simplified_current_page": 3}
```

---

## ğŸ” **Search Functionality**
### Course Search
`GET /courses/search`  
**Advanced Parameters**:
```http
/courses/search?query=math&fields=course_name,summary_text&fuzzy=true&skip=0&limit=5
```

### Vocabulary Search
`GET /courses/{course_id}/vocabulary/search`  
**Features**:
- Exact/partial term matching
- Definition searching
- Pagination

---

## ğŸ“– **Vocabulary System**
| Endpoint | Method | Description |
|----------|--------|-------------|
| `POST /api/create-vocabularies/{course_id}` | POST | Generate vocabulary from course |
| `GET /api/vocabularies/{course_id}/words` | GET | Get all vocabulary words |
| `GET /courses/{course_id}/vocabulary/search` | GET | Advanced term search |

**Vocabulary Object**:
```json
{
  "term": "photosynthesis",
  "definition": "Process by which plants convert light energy..."
}
```

---

## ğŸ›  **Technical Stack**
| Functionality | Technology |
|--------------|------------|
| PDF Processing | PyMuPDF |
| Image OCR | Pytesseract |
| Video Processing | FFmpeg + OpenCV |
| Text Generation | Ollama LLM |
| Vector Storage | JSON-serialized embeddings |

---

## âš ï¸ **Error Handling**
Standard error responses:
```json
{
  "detail": "Error message",
  "status_code": 400/404/500
}
```

---

This documentation covers all current endpoints with:
1. Clear parameter descriptions
2. Example requests/responses
3. Technical implementation details
4. Error handling specifications

=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================

Here's the comprehensive documentation for your StudyAI schema definitions:

---

# ğŸ“ StudyAI Schema Documentation

## ğŸ‘¤ **User Schemas**

### `UserBase`
```python
class UserBase(BaseModel):
    fullName: str
    email: str
    class_level: str
    password: str  # Will be hashed
    best_subjects: str
    learning_objectives: str
    academic_level: str
    statistic: int
```

### `UserCreate` (Registration)
- Inherits all fields from `UserBase`

### `User` (Response Model)
```python
class User(UserBase):
    id: int
    # Config enables ORM mode for SQLAlchemy
```

### Authentication
```python
class LoginRequest(BaseModel):
    email: str
    password: str

class TokenResponse(BaseModel):
    access_token: str
    token_type: str = "bearer"
```

---

## ğŸ“š **Course Schemas**

### Enums
```python
class CourseLevelEnum(str, Enum):
    beginner = "beginner"
    intermediate = "intermediate"
    advanced = "advanced"
```

### `CourseBase`
```python
class CourseBase(BaseModel):
    course_name: str
    original_text: Optional[str]
    simplified_text: Optional[str]
    summary_text: Optional[str]
    level: CourseLevelEnum
    estimated_completion_time: Optional[str]
    # Module structures
    summary_modules: List[Dict] = []
    simplified_modules: List[Dict] = []
    # Pagination
    simplified_module_pages: int = 0
    summary_module_pages: int = 0
    # Progress tracking
    simplified_current_page: int = 1
    summary_current_page: int = 1
    simplified_module_statistic: float = 0.0
    summary_modules_statistic: float = 0.0
    document_id: int
```

### `Course` (Full Response)
```python
class Course(CourseBase):
    id_course: int
    created_at: datetime
    quizzes: List['Quiz'] = []
    vocabularies: List['Vocabulary'] = []
```

---

## â“ **Quiz Schemas**

### Enums
```python
class QuizTypeEnum(str, Enum):
    qcm = "qcm"               # Multiple Choice
    texte = "texte"           # Free Text
    true_or_false = "true_or_false"
```

### `QuizBase`
```python
class QuizBase(BaseModel):
    course_id: int
    instruction: str
    question: str
    correct_answer: str
    choices: dict             # {"A": "Option 1", "B": "Option 2"}
    quiz_type: QuizTypeEnum
    level_of_difficulty: str  # Should be separate enum
    number_of_questions: int
```

### `Quiz` (Response)
```python
class Quiz(QuizBase):
    id: int
    created_at: datetime
    feedbacks: List['Feedback'] = []
```

---

## ğŸ“– **Vocabulary Schemas**

### `VocabularyBase`
```python
class VocabularyBase(BaseModel):
    course_id: int
    words: List[Dict] = []    # [{"term": "Photosynthesis", "definition": "..."}]
```

### `VocabularyWords` (Response)
```python
class VocabularyWords(BaseModel):
    words: List[Dict]
```

### Search Responses
```python
class VocabularySearchResult(BaseModel):
    term: str
    definition: str

class VocabularySearchResponse(BaseModel):
    results: List[Dict]
    pagination: Dict[str, int]
```

---

## ğŸ’¬ **Feedback Schema**

### `FeedbackBase`
```python
class FeedbackBase(BaseModel):
    quiz_id: int
    rating: int               # 1-5 scale
    comment: Optional[str]
```

### `Feedback` (Response)
```python
class Feedback(FeedbackBase):
    id: int
    created_at: datetime
```

---

## ğŸ“„ **Document Enums**

```python
class DocumentTypeEnum(str, Enum):
    pdf = "pdf"
    image = "image"
    video = "video"
```

---

### Key Features
1. **Progress Tracking**: Built-in statistics for course completion
2. **Modular Content**: Nested module structures for simplified/summary versions
3. **Type Safety**: Enums for fixed-value fields
4. **ORM Compatibility**: All models support SQLAlchemy conversion

### Example Usage
```python
# Creating a course
course_data = {
    "course_name": "Biology 101",
    "level": "beginner",
    "document_id": 123
}
CourseCreate(**course_data)
```
=======================================================================================================================================================================================
=======================================================================================================================================================================================
=======================================================================================================================================================================================


For large project this is the appropriate structure
study-ai-backend/
â”‚
â”œâ”€â”€ api/                          # API endpoints
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ courses.py                # Course management routes
â”‚   â”œâ”€â”€ documents.py              # File upload processing
â”‚   â”œâ”€â”€ quiz.py                   # Quiz generation routes
â”‚   â”œâ”€â”€ users.py                  # Authentication & user management
â”‚   â”œâ”€â”€ vocabulary.py             # Vocabulary endpoints
â”‚   â””â”€â”€ feedback.py               # Quiz feedback system
â”‚
â”œâ”€â”€ core/                         # Application core
â”‚   â”œâ”€â”€ config.py                 # App configuration
â”‚   â”œâ”€â”€ security.py               # Auth utilities
â”‚   â””â”€â”€ models.py                 # SQLAlchemy models
â”‚
â”œâ”€â”€ services/                     # Business logic
â”‚   â”œâ”€â”€ ai_processing/            # AI-related services
â”‚   â”‚   â”œâ”€â”€ content_processor.py  # Text processing
â”‚   â”‚   â”œâ”€â”€ quiz_generator.py     # Quiz creation
â”‚   â”‚   â””â”€â”€ vocabulary_builder.py # Term extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ course_service.py         # Course management
â”‚   â”œâ”€â”€ document_service.py       # File processing
â”‚   â”œâ”€â”€ feedback_service.py       # Feedback analysis
â”‚   â””â”€â”€ user_service.py           # User operations
â”‚
â”œâ”€â”€ schemas/                      # Pydantic models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ courses.py                # Course schemas
â”‚   â”œâ”€â”€ documents.py              # File schemas
â”‚   â”œâ”€â”€ users.py                  # User schemas
â”‚   â”œâ”€â”€ quiz.py                   # Quiz schemas
â”‚   â””â”€â”€ vocabulary.py             # Vocabulary schemas
â”‚
â”œâ”€â”€ utils/                        # Utilities
â”‚   â”œâ”€â”€ file_handlers/            # File processing
â”‚   â”‚   â”œâ”€â”€ pdf_handler.py        # PDF-specific
â”‚   â”‚   â”œâ”€â”€ image_handler.py      # Image processing
â”‚   â”‚   â””â”€â”€ video_handler.py      # Video processing
â”‚   â”‚
â”‚   â”œâ”€â”€ ai_utils/                 # AI helpers
â”‚   â”‚   â”œâ”€â”€ ollama_client.py      # LLM interactions
â”‚   â”‚   â””â”€â”€ embeddings.py         # Vector generation
â”‚   â”‚
â”‚   â”œâ”€â”€ database.py               # DB utilities
â”‚   â””â”€â”€ helpers.py                # General utilities
â”‚
â”œâ”€â”€ tests/                        # Testing
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â””â”€â”€ integration/              # Integration tests
â”‚
â”œâ”€â”€ static/                       # Static files
â”‚   â””â”€â”€ docs/                     # API documentation
â”‚
â”œâ”€â”€ temp_files/                   # Temporary storage
â”‚   â”œâ”€â”€ uploads/                  # User uploads
â”‚   â””â”€â”€ processed/                # Processed files
â”‚
â”œâ”€â”€ main.py                       # App entry point
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # Project documentation


# Connect to PostgreSQL (will prompt for password)
psql -U postgres -h localhost -p 5432

# When connected, run:
CREATE EXTENSION IF NOT EXISTS pg_trgm;

# Verify installation
\dx pg_trgm