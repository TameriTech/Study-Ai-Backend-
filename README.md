# üìö Study AI ‚Äì Plateforme d'apprentissage intelligente

![Python](https://img.shields.io/badge/python-3.12-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green)
![Status](https://img.shields.io/badge/status-en%20cours-yellow)

---

## üìë Table des mati√®res

- [Pr√©sentation](#-study-ai--plateforme-dapprentissage-intelligente)
- [Objectifs](#-objectifs)
- [Fonctionnalit√©s](#-fonctionnalit√©s-principales)
- [Technologies](#-technologies-utilis√©es)
- [Architecture](#-architecture-du-projet)
- [Structure](#-structure-du-projet-studyai)
- [Contraintes](#-contraintes--d√©fis)
- [Crit√®res de validation](#-crit√®res-de-validation)
- [Tests](#-tests)
- [Installation & Lancement](#-lancement-rapide-dev)
- [Fichier .env](#-exemple-de-fichier-env)
- [Contribution](#-contribuer)
- [Contact](#-auteurs--contribution)

---

## üß† Pr√©sentation

**Study AI** est une application innovante d√©velopp√©e par **Tameri Tech** pour transformer l'apprentissage des √©tudiants. Elle permet d'automatiser l'analyse de contenus p√©dagogiques (PDF, vid√©os) gr√¢ce √† l'intelligence artificielle, et g√©n√®re des fiches de r√©vision et des quiz personnalis√©s.

---

## üöÄ Objectifs

- Offrir une solution interactive pour r√©viser efficacement
- G√©n√©rer automatiquement des contenus √† partir de documents/vid√©os
- Am√©liorer la pertinence via l'IA et les retours utilisateurs

---

## üß© Fonctionnalit√©s principales

- üìÑ **PDF** : Importation, extraction, analyse de contenu
- üé• **Vid√©os** : Traitement image, reconnaissance vocale
- üñãÔ∏è **G√©n√©ration automatique** : Fiches de r√©vision, quiz personnalis√©s
- ü§ñ **IA & Feedback** : Am√©lioration continue, retours utilisateurs
- üìä **Dashboard interactif** : Acc√®s aux fichiers, fiches et statistiques

---

## üõ†Ô∏è Technologies utilis√©es

| Composant       | Technologies                                                  |
|----------------|---------------------------------------------------------------|
| Backend         | Python, FastAPI                                               |
| Frontend        | Kotlin (Java) *(optionnel)*                                   |
| Base de donn√©es | PostgreSQL                                                    |
| PDF             | PDFMiner, Apache Tika                                         |
| Vid√©o & Audio   | OpenCV, Google/Azure Speech-to-Text                           |
| IA              | OpenAI API (GPT-4), TensorFlow / PyTorch                      |

---

## üìä Architecture du projet

L'application suit une **architecture modulaire en couches** :

- Routes FastAPI (pdf, vid√©os, quiz, feedback, user)
- Services de traitement (OCR, NLP, IA, audio)
- Moteur de g√©n√©ration de quiz & fiches
- Syst√®me de feedback
- Base de donn√©es et mod√®les ORM

---

## üìÅ Structure du projet StudyAI

```bash

Study-Ai-Backend/
‚îÇ
‚îú‚îÄ‚îÄ api/                          # Contains all route/controller logic
‚îÇ   ‚îú‚îÄ‚îÄ courses.py
‚îÇ   ‚îú‚îÄ‚îÄ documents.py
‚îÇ   ‚îú‚îÄ‚îÄ feedback.py
‚îÇ   ‚îú‚îÄ‚îÄ quiz.py
‚îÇ   ‚îú‚îÄ‚îÄ segments.py
‚îÇ   ‚îú‚îÄ‚îÄ users.py
‚îÇ   ‚îî‚îÄ‚îÄ vocabulary.py
‚îÇ
‚îú‚îÄ‚îÄ database/                     # Database layer: connection, models, and schemas
‚îÇ   ‚îú‚îÄ‚îÄ db.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
‚îÇ
‚îú‚îÄ‚îÄ services/                     # Business logic and service layer
‚îÇ   ‚îú‚îÄ‚îÄ course_service.py
‚îÇ   ‚îú‚îÄ‚îÄ document_service.py
‚îÇ   ‚îú‚îÄ‚îÄ feedback_service.py
‚îÇ   ‚îú‚îÄ‚îÄ quiz_service.py
‚îÇ   ‚îú‚îÄ‚îÄ segment_service.py
‚îÇ   ‚îú‚îÄ‚îÄ users_services.py
‚îÇ   ‚îî‚îÄ‚îÄ vocabulary_services.py
‚îÇ
‚îú‚îÄ‚îÄ utils/                        # Utility functions for reusability
‚îÇ   ‚îú‚îÄ‚îÄ general_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ image_util.py
‚îÇ   ‚îú‚îÄ‚îÄ ollama_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ pdf_util.py
‚îÇ   ‚îî‚îÄ‚îÄ video_util.py
‚îÇ
‚îú‚îÄ‚îÄ temp_files/                   # Temporary storage for uploaded or processed files
‚îÇ   ‚îú‚îÄ‚îÄ images/                   # Temporary image files
‚îÇ   ‚îú‚îÄ‚îÄ pdf/                      # Temporary PDF documents
‚îÇ   ‚îî‚îÄ‚îÄ videos/                   # Temporary video files
‚îÇ
‚îú‚îÄ‚îÄ .env                          # Environment variables
‚îú‚îÄ‚îÄ .gitignore                    # Git ignored files/folders
‚îú‚îÄ‚îÄ api_documentation.md         # API documentation
‚îú‚îÄ‚îÄ main.py                       # Application entry point
‚îú‚îÄ‚îÄ README.md                     # Project description and instructions
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ TODO.md                       # Tasks and future enhancements
```

---

## ‚ö†Ô∏è Contraintes & d√©fis

- Gestion de PDF vari√©s (avec code, images, etc.)
- Qualit√© des vid√©os/audio parfois faible
- Temps de traitement de gros fichiers
- Interface √† la fois intuitive et riche

---

## ‚úÖ Crit√®res de validation

| Fonctionnalit√©       | Crit√®re attendu                                     |
|----------------------|------------------------------------------------------|
| PDF                  | Pr√©cision > 95%                                     |
| Vid√©o               | Concepts extraits dans > 90% des cas                 |
| Quiz                 | Pertinence du contenu g√©n√©r√©                       |
| Performance          | Temps de r√©ponse acceptable (m√™me fichiers lourds)   |
| Feedback utilisateur | Syst√®me intuitif, utilis√© activement                |

---

## üß™ Tests

- üî¨ **Unitaires** : PDFImporter, VideoAnalyzer, QuizGenerator
- üîó **Int√©gration** : Cha√Æne compl√®te (import ‚Üí quiz)
- üîç **Fonctionnels** : Cas d‚Äôusage r√©els
- üìä **Performance** : Scalabilit√©, charge
- üë• **Utilisateurs** : Feedback humain pour ajustement

---

## üèÅ Lancement rapide (dev)

```bash
# Cloner le projet
git clone https://github.com/TameriTech/Study-Ai-Backend-.git
cd study-ai

# Cr√©er l'environnement virtuel
python -m venv env
source env/Scripts/activate

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer l'application
uvicorn app.main:app --reload
```

---

## üîê Exemple de fichier `.env`

```env
OPENAI_API_KEY=sk-xxx
GOOGLE_SPEECH_API_KEY=xxx
DATABASE_URL=postgresql://user:password@localhost/studyai
```

---

## ü§ù Contribuer

Les contributions sont les bienvenues !

- Forkez le repo
- Cr√©ez une branche (`git checkout -b feature/ma-feature`)
- Commitez vos changements
- Push (`git push origin feature/ma-feature`)
- Ouvrez une pull request üöÄ

---

## üì¢ Auteurs & Contribution

**D√©velopp√© par :** Tameri Tech  
**Contact :** [tameri.tech25@gmail.com](mailto:tameri.tech25@gmail.com)

> Diagrammes UML disponibles dans le rapport d‚Äôanalyse officiel du projet.

---

**‚ú® Rejoignez la r√©volution de l'apprentissage intelligent avec Study AI.**

===========================================================================

USER
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ id
‚Ä¢ fullName
‚Ä¢ email
‚Ä¢ password
‚Ä¢ best_subjects
‚Ä¢ learning_objectives
‚Ä¢ class_level
‚Ä¢ academic_level
‚Ä¢ statistic
‚Ä¢ created_at
‚îÇ
‚îî‚îÄ‚îÄ‚îÄ 1:* ‚îÄ‚îÄ‚îÄ‚ñ∫ DOCUMENT
              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚Ä¢ id
              ‚Ä¢ title
              ‚Ä¢ type_document
              ‚Ä¢ original_filename
              ‚Ä¢ storage_path
              ‚Ä¢ original_text
              ‚Ä¢ uploaded_at
              ‚Ä¢ user_id
              ‚îÇ
              ‚îú‚îÄ‚îÄ‚îÄ 1:* ‚îÄ‚îÄ‚îÄ‚ñ∫ COURSE
              ‚îÇ            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚îÇ            ‚Ä¢ id
              ‚îÇ            ‚Ä¢ course_name
              ‚îÇ            ‚Ä¢ original_text
              ‚îÇ            ‚Ä¢ simplified_text
              ‚îÇ            ‚Ä¢ summary_text
              ‚îÇ            ‚Ä¢ level
              ‚îÇ            ‚Ä¢ estimated_completion_time
              ‚îÇ            ‚Ä¢ summary_module
              ‚îÇ            ‚Ä¢ simplified_modules
              ‚îÇ            ‚Ä¢ simplified_module_pages
              ‚îÇ            ‚Ä¢ summary_modules_pages
              ‚îÇ            ‚Ä¢ simplified_current_page
              ‚îÇ            ‚Ä¢ summary_current_page
              ‚îÇ            ‚Ä¢ simplified_module_statistic
              ‚îÇ            ‚Ä¢ summary_modules_statistic
              ‚îÇ            ‚Ä¢ document_id
              ‚îÇ            ‚Ä¢ created_at
              ‚îÇ            ‚îÇ
              ‚îÇ            ‚îú‚îÄ‚îÄ‚îÄ 1:* ‚îÄ‚îÄ‚îÄ‚ñ∫ QUIZ
              ‚îÇ            ‚îÇ            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚îÇ            ‚îÇ            ‚Ä¢ id
              ‚îÇ            ‚îÇ            ‚Ä¢ instruction
              ‚îÇ            ‚îÇ            ‚Ä¢ question
              ‚îÇ            ‚îÇ            ‚Ä¢ correct_answer
              ‚îÇ            ‚îÇ            ‚Ä¢ choices
              ‚îÇ            ‚îÇ            ‚Ä¢ quiz_type
              ‚îÇ            ‚îÇ            ‚Ä¢ level_of_difficulty
              ‚îÇ            ‚îÇ            ‚Ä¢ number_of_questions
              ‚îÇ            ‚îÇ            ‚Ä¢ created_at
              ‚îÇ            ‚îÇ            ‚Ä¢ course_id
              ‚îÇ            ‚îÇ            ‚îÇ
              ‚îÇ            ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ 1:1 ‚îÄ‚îÄ‚îÄ‚ñ∫ FEEDBACK
              ‚îÇ            ‚îÇ                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚îÇ            ‚îÇ                         ‚Ä¢ id
              ‚îÇ            ‚îÇ                         ‚Ä¢ rating
              ‚îÇ            ‚îÇ                         ‚Ä¢ comment
              ‚îÇ            ‚îÇ                         ‚Ä¢ created_at
              ‚îÇ            ‚îÇ                         ‚Ä¢ quiz_id
              ‚îÇ            ‚îÇ
              ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ 1:1 ‚îÄ‚îÄ‚îÄ‚ñ∫ VOCABULARY
              ‚îÇ                          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
              ‚îÇ                          ‚Ä¢ id
              ‚îÇ                          ‚Ä¢ words [{term:"", definition:""}]
              ‚îÇ                          ‚Ä¢ course_id
              ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ 1:* ‚îÄ‚îÄ‚îÄ‚ñ∫ SEGMENT
                          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                          ‚Ä¢ id
                          ‚Ä¢ raw_text
                          ‚Ä¢ embedding_vector
                          ‚Ä¢ created_at
                          ‚Ä¢ document_id


===========================================================================

Here's a clear and professional documentation for the `users_services` module based on your provided code. This can be added to your `api_documentation.md` or kept separately under a **Services Documentation** section.

---

## üß© `users_services.py` ‚Äì User Service Logic

This module handles **CRUD operations** and **authentication logic** for users.

### üîí Authentication

```python
def authenticate_user(db: Session, email: str, password: str)
```

- **Purpose**: Validates user credentials.
- **Parameters**:
  - `db`: SQLAlchemy DB session.
  - `email` *(str)*: User's email address.
  - `password` *(str)*: Plain-text password to verify.
- **Returns**: User object if credentials are valid, otherwise `None`.

---

### üßë Create User

```python
def create_user(db: Session, data: UserCreate)
```

- **Purpose**: Creates a new user after checking for email uniqueness.
- **Parameters**:
  - `db`: SQLAlchemy DB session.
  - `data`: Pydantic schema `UserCreate` containing user details.
- **Logic**:
  - Checks if the email already exists.
  - Hashes the password before storing.
- **Raises**: `HTTPException` with status `400` if email already exists.
- **Returns**: The newly created user object.

---

### üì• Get All Users

```python
def get_users(db: Session)
```

- **Purpose**: Fetches all users from the database.
- **Returns**: List of user objects.

---

### üë§ Get a Single User

```python
def get_user(db: Session, user_id: int)
```

- **Purpose**: Fetches a user by their ID.
- **Returns**: A single user object or `None` if not found.

---

### ‚úèÔ∏è Update User

```python
def update_user(db: Session, user: UserCreate, user_id: int)
```

- **Purpose**: Updates user fields with new data.
- **Parameters**:
  - `db`: SQLAlchemy DB session.
  - `user`: New user data (as `UserCreate` schema).
  - `user_id`: ID of the user to update.
- **Logic**: Dynamically updates fields using `model_dump()`.
- **Returns**: The updated user object or `None` if not found.

---

### ‚ùå Delete User

```python
def delete_user(db: Session, user_id: int)
```

- **Purpose**: Deletes a user by their ID.
- **Returns**: The deleted user object or `None` if not found.

---



===========================================================================

Here's the comprehensive documentation Documents module following your established style:

---

## üìÑ `documents services` ‚Äì Document Processing Service

Handles file uploads (PDFs, images, videos), text extraction, and initial processing pipeline including:
- File storage management
- Text extraction (OCR for images/videos)
- AI-powered summarization/simplification
- Automatic course creation
- Text segmentation with embeddings

---

### üìë **PDF Processing**
```python
async def extract_and_save_pdf(db: Session, file: UploadFile, user_id: int) -> dict
```

#### Features
- Validates PDF files
- Extracts raw text using PyMuPDF
- Generates AI summaries and simplifications
- Creates database records and initiates processing pipeline

#### Parameters
| Parameter | Type          | Description                          |
|-----------|---------------|--------------------------------------|
| `db`      | `Session`     | SQLAlchemy database session          |
| `file`    | `UploadFile`  | PDF file to process                  |
| `user_id` | `int`         | Owner's user ID                      |

#### Returns
```json
{
  "document_id": 123,
  "user_id": 456,
  "filename": "notes.pdf",
  "storage_path": "temp_files/pdf/20240101_123456_notes.pdf",
  "extracted_text": "Lorem ipsum...",
  "message": "PDF processed successfully..."
}
```

#### Error Cases
- `400 Bad Request`: Non-PDF file uploaded
- `500 Internal Server Error`: File processing failures

---

### üì∏ **Image Processing**
```python
async def extract_and_save_image(db: Session, file: UploadFile, user_id: int) -> dict
```

#### Features
- Accepts common image formats (JPEG, PNG, etc.)
- Performs OCR using Tesseract
- Parallel processing pipeline to PDF documents

#### Special Considerations
```python
# Requires Tesseract OCR installed:
# sudo apt install tesseract-ocr  # Linux
# brew install tesseract           # macOS
```

#### Error Cases
- `400 Bad Request`: Non-image file uploaded
- `500 Internal Server Error`: OCR processing failures

---

### üé• **Video Processing**
```python
async def extract_and_save_video(
    db: Session, 
    file: UploadFile, 
    user_id: int, 
    frames_per_second: int = 1
) -> dict
```

#### Features
- Supports MP4, MOV, AVI, MKV
- Frame extraction via FFmpeg
- Per-frame OCR processing
- Configurable FPS for performance/coverage tradeoff

#### Dependencies
```bash
# Requires FFmpeg:
# sudo apt install ffmpeg  # Linux
# brew install ffmpeg      # macOS
```

#### Error Cases
- `400 Bad Request`: Unsupported video format
- `500 Internal Server Error`: FFmpeg/OCR processing failures

---

### üîÑ **Common Processing Pipeline**
All methods follow this workflow:
1. File validation ‚Üí 2. Storage ‚Üí 3. Text extraction ‚Üí  
4. AI processing ‚Üí 5. Database records ‚Üí 6. Course/Segment creation

#### Shared Return Structure
All successful operations return:
- Document metadata
- Extracted text sample
- Processing confirmation
- Generated course ID

#### Error Handling
Uniform error responses with:
- Machine-readable status codes
- Human-friendly error messages
- Contextual details for debugging

---

### üõ† **Utility Functions**
| Function                | Description                                  |
|-------------------------|----------------------------------------------|
| `_save_to_temp()`       | Handles secure file storage with timestamped names |
| `_validate_file_type()` | Checks file extensions and MIME types        |

---



===========================================================================

Here's the comprehensive documentation for your `course_service.py` module:

---

## üìö `course_service.py` ‚Äì Course Management Service

Handles course creation, module processing, progress tracking, and search functionality for your learning platform.

---

### ‚ûï **Create Course**
```python
def create_course(
    db, 
    document_id: int,
    course_name: str,
    original_text: Optional[str] = None,
    simplified_text: Optional[str] = None,
    summary_text: Optional[str] = None,
    level: str = "beginner"
) -> Course
```

#### Features
- Automatically structures content into modules using AI
- Generates estimated completion time
- Creates both simplified and summary versions
- Calculates initial pagination statistics

#### AI Processing Pipeline
1. **Module Generation**:
   ```python
   simplified_modules = generate_from_ollama(prompt)  # For both simplified/summary
   ```
2. **Time Estimation**:
   ```python
   estimated_time = generate_from_ollama("...text...")  # <12 character response
   ```

#### Returns
`Course` object with:
- Structured modules (JSON)
- Pagination counts
- Original/processed content
- Completion metrics

---

### üîç **Module Retrieval**
| Function | Description |
|----------|-------------|
| `get_simplified_modules()` | Gets simplified modules by document ID |
| `get_summary_modules()` | Gets summary modules by document ID |
| `get_simplified_modules_by_course_id()` | Gets modules by course ID |
| `get_summary_modules_by_course_id()` | Gets summary by course ID |

**All functions return**:  
`List[Dict]` or `None` if not found

---

### üìà **Progress Tracking**
```python
def update_simplified_progress(db, course_id, current_page) -> Course
def update_summary_progress(db, course_id, current_page) -> Course
```

#### Features
- Auto-calculates completion percentage
- Prevents page overflow
- Updates both current page and statistics

**Example**:
```python
# Updates page 3 of 10 ‚Üí 30% completion
update_simplified_progress(db, 123, 3)  
```

---

### üóÇ **Course Access**
| Function | Description |
|----------|-------------|
| `get_course_from_db()` | Gets full course by ID |
| `get_user_courses()` | Lists all courses for a user |

**Returns**:  
- Single `Course` object or `None`
- List of courses (SQLAlchemy objects)

---

### üîé **Advanced Search**
```python
def search_courses(
    db,
    search_query: str,
    min_query_length=2,
    limit=10,
    skip=0,
    search_fields=None,
    fuzzy_match=False
) -> dict
```

#### Features
- Field-specific searching (default: course_name)
- Fuzzy matching (PG trigram similarity)
- Pagination support

**Search Options**:
```python
# Exact match on multiple fields
search_courses(db, "math", search_fields=["course_name", "original_text"])

# Fuzzy matching
search_courses(db, "algebr", fuzzy_match=True)
```

**Return Structure**:
```json
{
  "results": [Course1, Course2],
  "pagination": {
    "total": 15,
    "returned": 2,
    "skip": 0,
    "limit": 10
  }
}
```

#### Error Cases
- `400 Bad Request`: Query too short
- `400 Bad Request`: Invalid search fields

---

### üõ† **Internal Utilities**
| Function | Description |
|----------|-------------|
| `parse_modules()` | Converts AI output to structured modules |
| `_calculate_progress()` | Handles percentage calculations |

---

===========================================================================

Here's the documentation for your `segment_service.py` module:

---

## üîç `segment_service.py` ‚Äì Text Segmentation & Embedding Service

Handles text chunking and vector embedding generation for semantic search and content analysis.

---

### ‚öôÔ∏è **Core Function**
```python
def process_segments(
    db: Session, 
    document_id: int, 
    text: str, 
    chunk_size: int = 1000
) -> int
```

#### Features
- Splits text into manageable chunks
- Generates 384-dimension embeddings using `all-MiniLM-L6-v2` model
- Stores embeddings as JSON-serialized arrays
- Skips empty/whitespace chunks
- Returns count of created segments

#### Parameters
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `db` | `Session` | - | SQLAlchemy session |
| `document_id` | `int` | - | Parent document ID |
| `text` | `str` | - | Raw content to process |
| `chunk_size` | `int` | 1000 | Character length per segment |

#### Technical Details
```python
# Embedding Generation
embedding = model.encode(text_chunk)  # Returns numpy array
vector = json.dumps(embedding.tolist())  # Serialized storage

# Chunking Logic
chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
```

#### Returns
Integer count of successfully created segments

---

### üßÆ **Embedding Specifications**
| Model | Dimensions | Size | Best For |
|-------|------------|------|----------|
| `all-MiniLM-L6-v2` | 384 | ~80MB | Fast semantic search |

---

### üíæ **Database Storage**
```python
class Segment(BaseModel):
    embedding_vector: str  # JSON string of float array
    raw_text: str         # Original chunk content
    document_id: int      # Foreign key
```

#### Retrieval Example
```python
# Get embedding back to numpy array
segment = db.query(Segment).first()
embedding = np.array(json.loads(segment.embedding_vector))
```

---

### ‚ö†Ô∏è **Requirements**
1. SentenceTransformers installed:
   ```bash
   pip install sentence-transformers
   ```
2. NumPy for array handling

---

### üìä **Performance Considerations**
- **Chunk Size**: 500-1500 chars recommended
- **Memory**: ~1MB per 1000 segments
- **Processing**: ~100ms per chunk on CPU

Would you like me to:
1. Add examples of querying with embeddings?
2. Document the model selection tradeoffs?
3. Include error handling recommendations?

### üîÑ **Data Flow**
1. Document uploaded ‚Üí 2. Course created ‚Üí 3. Modules generated ‚Üí  
4. User interacts ‚Üí 5. Progress updated ‚Üí 6. Searchable content

===========================================================================

Here's the comprehensive documentation for your Vocabulary API endpoints:

---

## üìñ Vocabulary API Endpoints

### **Base URL**
`/api/vocabularies`

---

### ‚ûï Create Vocabulary Entry
`POST /api/create-vocabularies/{course_id}/`

#### **Description**
Creates a new vocabulary entry for a specific course.

#### **Parameters**
| Parameter | Type | Location | Required | Description |
|-----------|------|----------|----------|-------------|
| `course_id` | integer | path | Yes | ID of the associated course |

#### **Responses**
- `200 OK`: Returns the created vocabulary entry
  ```json
  {
    "id": 1,
    "words": [
      {"term": "algorithm", "definition": "A set of rules..."}
    ],
    "course_id": 5
  }
  ```
- `500 Internal Server Error`: Creation failed
  ```json
  {
    "detail": "Error message"
  }
  ```

---

### üìö Get Vocabulary Words
`GET /api/vocabularies/{course_id}/words`

#### **Description**
Retrieves all vocabulary words for a specific course.

#### **Parameters**
| Parameter | Type | Location | Required | Description |
|-----------|------|----------|----------|-------------|
| `course_id` | integer | path | Yes | ID of the course |

#### **Responses**
- `200 OK`: Returns vocabulary words
  ```json
  {
    "words": [
      {"term": "variable", "definition": "A storage location..."},
      {"term": "function", "definition": "A reusable code block..."}
    ]
  }
  ```
- `500 Internal Server Error`: Retrieval failed
  ```json
  {
    "detail": "Error retrieving words: [error details]"
  }
  ```

---

### üîç Search Vocabulary
`GET /api/vocabularies/{course_id}/search`

#### **Description**
Searches for vocabulary terms within a course.

#### **Parameters**
| Parameter | Type | Location | Required | Description |
|-----------|------|----------|----------|-------------|
| `course_id` | integer | path | Yes | ID of the course |
| `keyword` | string | query | Yes | Search term (min 1 character) |

#### **Responses**
- `200 OK`: Returns matching vocabulary words
  ```json
  {
    "words": [
      {"term": "database", "definition": "An organized collection..."}
    ]
  }
  ```
- `500 Internal Server Error`: Search failed
  ```json
  {
    "detail": "Search error: [error details]"
  }
  ```

---

### üè∑Ô∏è Data Schemas
#### **Vocabulary**
```json
{
  "id": "integer",
  "words": [
    {
      "term": "string",
      "definition": "string"
    }
  ],
  "course_id": "integer"
}
```

#### **VocabularyWords**
```json
{
  "words": [
    {
      "term": "string",
      "definition": "string"
    }
  ]
}
```

---

### Error Handling
All endpoints return standardized error responses:
- `500` status code for server errors
- JSON-formatted error details
- Original error message in development mode

---

### Example Usage
```bash
# Create vocabulary
curl -X POST http://localhost:8000/api/create-vocabularies/5/

# Get words
curl http://localhost:8000/api/vocabularies/5/words

# Search words
curl http://localhost:8000/api/vocabularies/5/search?keyword=variable
```


===========================================================================

Here's the comprehensive API documentation for your StudyAI backend routes:

---

# üìö StudyAI API Documentation

## üîê **Authentication**
`POST /api/login`  
**Description**: Authenticate user and get JWT token  
**Request Body**:
```json
{
  "email": "user@example.com",
  "password": "securepassword"
}
```
**Response**:
```json
{
  "access_token": "eyJhbGci...",
  "token_type": "bearer"
}
```

---

## üë• **User Management**
| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/api/register` | POST | Register new user | `UserCreate` schema |
| `/api/get-users` | GET | List all users | - |
| `/api/get-user/{id}` | GET | Get user by ID | `id` (path) |
| `/api/user/update/{id}` | PUT | Update user | `id` (path), `UserCreate` schema |
| `/api/delete/user/{id}` | DELETE | Delete user | `id` (path) |

---

## üìÑ **Document Processing**
### PDF Processing
`POST /api/extract-pdf-text`  
**Description**: Upload and process PDF  
**Parameters**:
- `file`: PDF file (UploadFile)
- `user_id`: Owner ID (query)

**Response**:
```json
{
  "document_id": 123,
  "extracted_text": "First 100 chars...",
  "storage_path": "temp_files/pdf/...",
  "simplified_text": "Simplified version...",
  "summary_text": "Condensed summary..."
}
```

### Image Processing
`POST /api/extract-text-from-image`  
**Tech Stack**: Uses Pytesseract for OCR  
**Error Cases**:
- `400`: Non-image file
- `500`: OCR processing failure

### Video Processing
`POST /api/extract-text-from-video`  
**Tech Stack**: FFmpeg (frame extraction) + Pytesseract (OCR)  
**Parameters**:
- `frames_per_second`: 1-10 (default: 1)

---

## üìö **Course Management**
### Course Retrieval
| Endpoint | Description | Response |
|----------|-------------|----------|
| `GET /api/get-course/{course_id}` | Get full course | `Course` object |
| `GET /api/user/{user_id}/courses` | List user's courses | `List[Course]` |

### Module Access
```mermaid
graph LR
    A[Document] --> B(Simplified Modules)
    A --> C(Summary Modules)
    B --> D[GET /courses-doc/{id}/simplified-modules]
    C --> E[GET /courses-doc/{id}/summary-modules]
```

### Progress Tracking
`PUT /api/course/{course_id}/update-simplified-progress`  
**Body**:
```json
{"simplified_current_page": 3}
```

---

## üîç **Search Functionality**
### Course Search
`GET /courses/search`  
**Advanced Parameters**:
```http
/courses/search?query=math&fields=course_name,summary_text&fuzzy=true&skip=0&limit=5
```

### Vocabulary Search
`GET /courses/{course_id}/vocabulary/search`  
**Features**:
- Exact/partial term matching
- Definition searching
- Pagination

---

## üìñ **Vocabulary System**
| Endpoint | Method | Description |
|----------|--------|-------------|
| `POST /api/create-vocabularies/{course_id}` | POST | Generate vocabulary from course |
| `GET /api/vocabularies/{course_id}/words` | GET | Get all vocabulary words |
| `GET /courses/{course_id}/vocabulary/search` | GET | Advanced term search |

**Vocabulary Object**:
```json
{
  "term": "photosynthesis",
  "definition": "Process by which plants convert light energy..."
}
```

---

## üõ† **Technical Stack**
| Functionality | Technology |
|--------------|------------|
| PDF Processing | PyMuPDF |
| Image OCR | Pytesseract |
| Video Processing | FFmpeg + OpenCV |
| Text Generation | Ollama LLM |
| Vector Storage | JSON-serialized embeddings |

---

## ‚ö†Ô∏è **Error Handling**
Standard error responses:
```json
{
  "detail": "Error message",
  "status_code": 400/404/500
}
```

---



===========================================================================

Here's the comprehensive documentation for your StudyAI schema definitions:

---

# üìù StudyAI Schema Documentation

## üë§ **User Schemas**

### `UserBase`
```python
class UserBase(BaseModel):
    fullName: str
    email: str
    class_level: str
    password: str  # Will be hashed
    best_subjects: str
    learning_objectives: str
    academic_level: str
    statistic: int
```

### `UserCreate` (Registration)
- Inherits all fields from `UserBase`

### `User` (Response Model)
```python
class User(UserBase):
    id: int
    # Config enables ORM mode for SQLAlchemy
```

### Authentication
```python
class LoginRequest(BaseModel):
    email: str
    password: str

class TokenResponse(BaseModel):
    access_token: str
    token_type: str = "bearer"
```

---

## üìö **Course Schemas**

### Enums
```python
class CourseLevelEnum(str, Enum):
    beginner = "beginner"
    intermediate = "intermediate"
    advanced = "advanced"
```

### `CourseBase`
```python
class CourseBase(BaseModel):
    course_name: str
    original_text: Optional[str]
    simplified_text: Optional[str]
    summary_text: Optional[str]
    level: CourseLevelEnum
    estimated_completion_time: Optional[str]
    # Module structures
    summary_modules: List[Dict] = []
    simplified_modules: List[Dict] = []
    # Pagination
    simplified_module_pages: int = 0
    summary_module_pages: int = 0
    # Progress tracking
    simplified_current_page: int = 1
    summary_current_page: int = 1
    simplified_module_statistic: float = 0.0
    summary_modules_statistic: float = 0.0
    document_id: int
```

### `Course` (Full Response)
```python
class Course(CourseBase):
    id_course: int
    created_at: datetime
    quizzes: List['Quiz'] = []
    vocabularies: List['Vocabulary'] = []
```

---

## ‚ùì **Quiz Schemas**

### Enums
```python
class QuizTypeEnum(str, Enum):
    qcm = "qcm"               # Multiple Choice
    texte = "texte"           # Free Text
    true_or_false = "true_or_false"
```

### `QuizBase`
```python
class QuizBase(BaseModel):
    course_id: int
    instruction: str
    question: str
    correct_answer: str
    choices: dict             # {"A": "Option 1", "B": "Option 2"}
    quiz_type: QuizTypeEnum
    level_of_difficulty: str  # Should be separate enum
    number_of_questions: int
```

### `Quiz` (Response)
```python
class Quiz(QuizBase):
    id: int
    created_at: datetime
    feedbacks: List['Feedback'] = []
```

---

## üìñ **Vocabulary Schemas**

### `VocabularyBase`
```python
class VocabularyBase(BaseModel):
    course_id: int
    words: List[Dict] = []    # [{"term": "Photosynthesis", "definition": "..."}]
```

### `VocabularyWords` (Response)
```python
class VocabularyWords(BaseModel):
    words: List[Dict]
```

### Search Responses
```python
class VocabularySearchResult(BaseModel):
    term: str
    definition: str

class VocabularySearchResponse(BaseModel):
    results: List[Dict]
    pagination: Dict[str, int]
```

---

## üí¨ **Feedback Schema**

### `FeedbackBase`
```python
class FeedbackBase(BaseModel):
    quiz_id: int
    rating: int               # 1-5 scale
    comment: Optional[str]
```

### `Feedback` (Response)
```python
class Feedback(FeedbackBase):
    id: int
    created_at: datetime
```

---

## üìÑ **Document Enums**

```python
class DocumentTypeEnum(str, Enum):
    pdf = "pdf"
    image = "image"
    video = "video"
```

---

### Key Features
1. **Progress Tracking**: Built-in statistics for course completion
2. **Modular Content**: Nested module structures for simplified/summary versions
3. **Type Safety**: Enums for fixed-value fields
4. **ORM Compatibility**: All models support SQLAlchemy conversion

### Example Usage
```python
# Creating a course
course_data = {
    "course_name": "Biology 101",
    "level": "beginner",
    "document_id": 123
}
CourseCreate(**course_data)
```


===========================================================================